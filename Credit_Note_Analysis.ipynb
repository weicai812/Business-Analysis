{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPY9mPaYnN0DvF8CcylkuLL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install PyPDF2\n","\n","import PyPDF2\n","\n","# Replace 'CreditNoteAll_PS250300014SLF20.pdf' with the actual path to your PDF file\n","pdf_file_path = 'CreditNoteAll_PS250300014SLF20.pdf'\n","\n","with open(pdf_file_path, 'rb') as pdf_file:\n","  pdf_reader = PyPDF2.PdfReader(pdf_file)\n","  num_pages = len(pdf_reader.pages)\n","\n","  for page_num in range(num_pages):\n","    page = pdf_reader.pages[page_num]\n","    text = page.extract_text()\n","    print(f\"Text from page {page_num + 1}:\")\n","text\n"],"metadata":{"id":"SqttU4s4wHFh","colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1742953091776,"user_tz":-480,"elapsed":14213,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"f43c3ba9-2aef-4824-829a-af7031b3e238"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/232.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n","Text from page 1:\n","Text from page 2:\n","Text from page 3:\n","Text from page 4:\n","Text from page 5:\n","Text from page 6:\n","Text from page 7:\n","Text from page 8:\n","Text from page 9:\n","Text from page 10:\n","Text from page 11:\n","Text from page 12:\n","Text from page 13:\n","Text from page 14:\n","Text from page 15:\n","Text from page 16:\n","Text from page 17:\n","Text from page 18:\n","Text from page 19:\n","Text from page 20:\n","Text from page 21:\n","Text from page 22:\n","Text from page 23:\n","Text from page 24:\n","Text from page 25:\n","Text from page 26:\n","Text from page 27:\n","Text from page 28:\n","Text from page 29:\n","Text from page 30:\n","Text from page 31:\n","Text from page 32:\n","Text from page 33:\n","Text from page 34:\n","Text from page 35:\n","Text from page 36:\n","Text from page 37:\n","Text from page 38:\n","Text from page 39:\n","Text from page 40:\n","Text from page 41:\n","Text from page 42:\n","Text from page 43:\n","Text from page 44:\n","Text from page 45:\n","Text from page 46:\n","Text from page 47:\n","Text from page 48:\n","Text from page 49:\n","Text from page 50:\n","Text from page 51:\n","Text from page 52:\n","Text from page 53:\n","Text from page 54:\n","Text from page 55:\n","Text from page 56:\n","Text from page 57:\n","Text from page 58:\n","Text from page 59:\n","Text from page 60:\n","Text from page 61:\n","Text from page 62:\n","Text from page 63:\n","Text from page 64:\n","Text from page 65:\n","Text from page 66:\n","Text from page 67:\n","Text from page 68:\n","Text from page 69:\n","Text from page 70:\n","Text from page 71:\n","Text from page 72:\n","Text from page 73:\n","Text from page 74:\n","Text from page 75:\n","Text from page 76:\n","Text from page 77:\n","Text from page 78:\n","Text from page 79:\n","Text from page 80:\n","Text from page 81:\n","Text from page 82:\n","Text from page 83:\n","Text from page 84:\n","Text from page 85:\n","Text from page 86:\n","Text from page 87:\n","Text from page 88:\n","Text from page 89:\n","Text from page 90:\n","Text from page 91:\n","Text from page 92:\n","Text from page 93:\n","Text from page 94:\n","Text from page 95:\n","Text from page 96:\n","Text from page 97:\n","Text from page 98:\n","Text from page 99:\n","Text from page 100:\n","Text from page 101:\n","Text from page 102:\n","Text from page 103:\n","Text from page 104:\n","Text from page 105:\n","Text from page 106:\n","Text from page 107:\n","Text from page 108:\n","Text from page 109:\n","Text from page 110:\n","Text from page 111:\n","Text from page 112:\n","Text from page 113:\n","Text from page 114:\n","Text from page 115:\n","Text from page 116:\n","Text from page 117:\n","Text from page 118:\n","Text from page 119:\n","Text from page 120:\n","Text from page 121:\n","Text from page 122:\n","Text from page 123:\n","Text from page 124:\n","Text from page 125:\n","Text from page 126:\n","Text from page 127:\n","Text from page 128:\n","Text from page 129:\n","Text from page 130:\n","Text from page 131:\n","Text from page 132:\n","Text from page 133:\n","Text from page 134:\n","Text from page 135:\n","Text from page 136:\n","Text from page 137:\n","Text from page 138:\n","Text from page 139:\n","Text from page 140:\n","Text from page 141:\n","Text from page 142:\n","Text from page 143:\n","Text from page 144:\n","Text from page 145:\n","Text from page 146:\n","Text from page 147:\n","Text from page 148:\n","Text from page 149:\n","Text from page 150:\n","Text from page 151:\n","Text from page 152:\n","Text from page 153:\n","Text from page 154:\n","Text from page 155:\n","Text from page 156:\n","Text from page 157:\n","Text from page 158:\n","Text from page 159:\n","Text from page 160:\n","Text from page 161:\n","Text from page 162:\n","Text from page 163:\n","Text from page 164:\n","Text from page 165:\n","Text from page 166:\n","Text from page 167:\n","Text from page 168:\n","Text from page 169:\n","Text from page 170:\n","Text from page 171:\n","Text from page 172:\n","Text from page 173:\n","Text from page 174:\n","Text from page 175:\n","Text from page 176:\n","Text from page 177:\n","Text from page 178:\n","Text from page 179:\n","Text from page 180:\n","Text from page 181:\n","Text from page 182:\n","Text from page 183:\n","Text from page 184:\n","Text from page 185:\n","Text from page 186:\n","Text from page 187:\n","Text from page 188:\n","Text from page 189:\n","Text from page 190:\n","Text from page 191:\n","Text from page 192:\n","Text from page 193:\n","Text from page 194:\n","Text from page 195:\n","Text from page 196:\n","Text from page 197:\n","Text from page 198:\n","Text from page 199:\n","Text from page 200:\n","Text from page 201:\n","Text from page 202:\n","Text from page 203:\n","Text from page 204:\n","Text from page 205:\n","Text from page 206:\n","Text from page 207:\n","Text from page 208:\n","Text from page 209:\n","Text from page 210:\n","Text from page 211:\n","Text from page 212:\n","Text from page 213:\n","Text from page 214:\n","Text from page 215:\n","Text from page 216:\n","Text from page 217:\n","Text from page 218:\n","Text from page 219:\n","Text from page 220:\n","Text from page 221:\n","Text from page 222:\n","Text from page 223:\n","Text from page 224:\n","Text from page 225:\n","Text from page 226:\n","Text from page 227:\n","Text from page 228:\n","Text from page 229:\n","Text from page 230:\n","Text from page 231:\n","Text from page 232:\n","Text from page 233:\n","Text from page 234:\n","Text from page 235:\n","Text from page 236:\n","Text from page 237:\n","Text from page 238:\n","Text from page 239:\n","Text from page 240:\n","Text from page 241:\n","Text from page 242:\n","Text from page 243:\n","Text from page 244:\n","Text from page 245:\n","Text from page 246:\n","Text from page 247:\n","Text from page 248:\n","Text from page 249:\n","Text from page 250:\n","Text from page 251:\n","Text from page 252:\n","Text from page 253:\n","Text from page 254:\n","Text from page 255:\n","Text from page 256:\n","Text from page 257:\n","Text from page 258:\n","Text from page 259:\n","Text from page 260:\n","Text from page 261:\n","Text from page 262:\n","Text from page 263:\n","Text from page 264:\n","Text from page 265:\n","Text from page 266:\n","Text from page 267:\n","Text from page 268:\n","Text from page 269:\n","Text from page 270:\n","Text from page 271:\n","Text from page 272:\n","Text from page 273:\n","Text from page 274:\n","Text from page 275:\n","Text from page 276:\n","Text from page 277:\n","Text from page 278:\n","Text from page 279:\n","Text from page 280:\n","Text from page 281:\n","Text from page 282:\n","Text from page 283:\n","Text from page 284:\n","Text from page 285:\n","Text from page 286:\n","Text from page 287:\n","Text from page 288:\n","Text from page 289:\n","Text from page 290:\n","Text from page 291:\n","Text from page 292:\n","Text from page 293:\n","Text from page 294:\n","Text from page 295:\n","Text from page 296:\n","Text from page 297:\n","Text from page 298:\n","Text from page 299:\n","Text from page 300:\n","Text from page 301:\n","Text from page 302:\n","Text from page 303:\n","Text from page 304:\n","Text from page 305:\n","Text from page 306:\n","Text from page 307:\n","Text from page 308:\n","Text from page 309:\n","Text from page 310:\n","Text from page 311:\n","Text from page 312:\n","Text from page 313:\n","Text from page 314:\n","Text from page 315:\n","Text from page 316:\n","Text from page 317:\n","Text from page 318:\n","Text from page 319:\n","Text from page 320:\n","Text from page 321:\n","Text from page 322:\n","Text from page 323:\n","Text from page 324:\n","Text from page 325:\n","Text from page 326:\n","Text from page 327:\n","Text from page 328:\n","Text from page 329:\n","Text from page 330:\n","Text from page 331:\n","Text from page 332:\n","Text from page 333:\n","Text from page 334:\n","Text from page 335:\n","Text from page 336:\n","Text from page 337:\n","Text from page 338:\n","Text from page 339:\n","Text from page 340:\n","Text from page 341:\n","Text from page 342:\n","Text from page 343:\n","Text from page 344:\n","Text from page 345:\n","Text from page 346:\n","Text from page 347:\n","Text from page 348:\n","Text from page 349:\n","Text from page 350:\n","Text from page 351:\n","Text from page 352:\n","Text from page 353:\n","Text from page 354:\n","Text from page 355:\n","Text from page 356:\n","Text from page 357:\n","Text from page 358:\n","Text from page 359:\n","Text from page 360:\n","Text from page 361:\n","Text from page 362:\n","Text from page 363:\n","Text from page 364:\n","Text from page 365:\n","Text from page 366:\n","Text from page 367:\n","Text from page 368:\n","Text from page 369:\n","Text from page 370:\n","Text from page 371:\n","Text from page 372:\n","Text from page 373:\n","Text from page 374:\n","Text from page 375:\n","Text from page 376:\n","Text from page 377:\n","Text from page 378:\n","Text from page 379:\n","Text from page 380:\n","Text from page 381:\n","Text from page 382:\n","Text from page 383:\n","Text from page 384:\n","Text from page 385:\n","Text from page 386:\n","Text from page 387:\n","Text from page 388:\n","Text from page 389:\n","Text from page 390:\n","Text from page 391:\n","Text from page 392:\n","Text from page 393:\n","Text from page 394:\n","Text from page 395:\n","Text from page 396:\n","Text from page 397:\n","Text from page 398:\n","Text from page 399:\n","Text from page 400:\n","Text from page 401:\n","Text from page 402:\n","Text from page 403:\n","Text from page 404:\n","Text from page 405:\n","Text from page 406:\n","Text from page 407:\n","Text from page 408:\n","Text from page 409:\n","Text from page 410:\n","Text from page 411:\n","Text from page 412:\n","Text from page 413:\n","Text from page 414:\n","Text from page 415:\n","Text from page 416:\n","Text from page 417:\n","Text from page 418:\n","Text from page 419:\n","Text from page 420:\n","Text from page 421:\n","Text from page 422:\n","Text from page 423:\n","Text from page 424:\n","Text from page 425:\n","Text from page 426:\n","Text from page 427:\n","Text from page 428:\n","Text from page 429:\n","Text from page 430:\n","Text from page 431:\n","Text from page 432:\n","Text from page 433:\n","Text from page 434:\n","Text from page 435:\n","Text from page 436:\n","Text from page 437:\n","Text from page 438:\n","Text from page 439:\n","Text from page 440:\n","Text from page 441:\n","Text from page 442:\n","Text from page 443:\n","Text from page 444:\n","Text from page 445:\n","Text from page 446:\n","Text from page 447:\n","Text from page 448:\n","Text from page 449:\n","Text from page 450:\n","Text from page 451:\n","Text from page 452:\n","Text from page 453:\n","Text from page 454:\n","Text from page 455:\n","Text from page 456:\n","Text from page 457:\n","Text from page 458:\n","Text from page 459:\n","Text from page 460:\n","Text from page 461:\n","Text from page 462:\n","Text from page 463:\n","Text from page 464:\n","Text from page 465:\n","Text from page 466:\n","Text from page 467:\n","Text from page 468:\n","Text from page 469:\n","Text from page 470:\n","Text from page 471:\n","Text from page 472:\n","Text from page 473:\n","Text from page 474:\n","Text from page 475:\n","Text from page 476:\n","Text from page 477:\n","Text from page 478:\n","Text from page 479:\n","Text from page 480:\n","Text from page 481:\n","Text from page 482:\n","Text from page 483:\n","Text from page 484:\n","Text from page 485:\n","Text from page 486:\n","Text from page 487:\n","Text from page 488:\n","Text from page 489:\n","Text from page 490:\n","Text from page 491:\n","Text from page 492:\n","Text from page 493:\n","Text from page 494:\n","Text from page 495:\n","Text from page 496:\n","Text from page 497:\n","Text from page 498:\n","Text from page 499:\n","Text from page 500:\n","Text from page 501:\n","Text from page 502:\n","Text from page 503:\n","Text from page 504:\n","Text from page 505:\n","Text from page 506:\n","Text from page 507:\n","Text from page 508:\n","Text from page 509:\n","Text from page 510:\n","Text from page 511:\n","Text from page 512:\n","Text from page 513:\n","Text from page 514:\n","Text from page 515:\n","Text from page 516:\n","Text from page 517:\n","Text from page 518:\n","Text from page 519:\n","Text from page 520:\n","Text from page 521:\n","Text from page 522:\n","Text from page 523:\n","Text from page 524:\n","Text from page 525:\n","Text from page 526:\n","Text from page 527:\n","Text from page 528:\n","Text from page 529:\n","Text from page 530:\n","Text from page 531:\n","Text from page 532:\n","Text from page 533:\n","Text from page 534:\n","Text from page 535:\n","Text from page 536:\n","Text from page 537:\n","Text from page 538:\n","Text from page 539:\n","Text from page 540:\n","Text from page 541:\n","Text from page 542:\n","Text from page 543:\n","Text from page 544:\n","Text from page 545:\n","Text from page 546:\n","Text from page 547:\n","Text from page 548:\n","Text from page 549:\n","Text from page 550:\n","Text from page 551:\n","Text from page 552:\n","Text from page 553:\n","Text from page 554:\n","Text from page 555:\n","Text from page 556:\n","Text from page 557:\n","Text from page 558:\n","Text from page 559:\n","Text from page 560:\n","Text from page 561:\n","Text from page 562:\n","Text from page 563:\n","Text from page 564:\n","Text from page 565:\n","Text from page 566:\n","Text from page 567:\n","Text from page 568:\n","Text from page 569:\n","Text from page 570:\n","Text from page 571:\n","Text from page 572:\n","Text from page 573:\n","Text from page 574:\n","Text from page 575:\n","Text from page 576:\n","Text from page 577:\n","Text from page 578:\n","Text from page 579:\n","Text from page 580:\n","Text from page 581:\n","Text from page 582:\n","Text from page 583:\n","Text from page 584:\n","Text from page 585:\n","Text from page 586:\n","Text from page 587:\n","Text from page 588:\n","Text from page 589:\n","Text from page 590:\n","Text from page 591:\n","Text from page 592:\n","Text from page 593:\n","Text from page 594:\n","Text from page 595:\n","Text from page 596:\n","Text from page 597:\n","Text from page 598:\n","Text from page 599:\n","Text from page 600:\n","Text from page 601:\n","Text from page 602:\n","Text from page 603:\n","Text from page 604:\n","Text from page 605:\n","Text from page 606:\n","Text from page 607:\n","Text from page 608:\n","Text from page 609:\n","Text from page 610:\n","Text from page 611:\n","Text from page 612:\n","Text from page 613:\n","Text from page 614:\n","Text from page 615:\n","Text from page 616:\n","Text from page 617:\n","Text from page 618:\n","Text from page 619:\n","Text from page 620:\n","Text from page 621:\n","Text from page 622:\n","Text from page 623:\n","Text from page 624:\n","Text from page 625:\n","Text from page 626:\n","Text from page 627:\n","Text from page 628:\n","Text from page 629:\n","Text from page 630:\n","Text from page 631:\n","Text from page 632:\n","Text from page 633:\n","Text from page 634:\n","Text from page 635:\n","Text from page 636:\n","Text from page 637:\n","Text from page 638:\n","Text from page 639:\n","Text from page 640:\n","Text from page 641:\n","Text from page 642:\n","Text from page 643:\n","Text from page 644:\n","Text from page 645:\n","Text from page 646:\n","Text from page 647:\n","Text from page 648:\n","Text from page 649:\n","Text from page 650:\n","Text from page 651:\n","Text from page 652:\n","Text from page 653:\n","Text from page 654:\n","Text from page 655:\n","Text from page 656:\n","Text from page 657:\n","Text from page 658:\n","Text from page 659:\n","Text from page 660:\n","Text from page 661:\n","Text from page 662:\n","Text from page 663:\n","Text from page 664:\n","Text from page 665:\n","Text from page 666:\n","Text from page 667:\n","Text from page 668:\n","Text from page 669:\n","Text from page 670:\n","Text from page 671:\n","Text from page 672:\n","Text from page 673:\n","Text from page 674:\n","Text from page 675:\n","Text from page 676:\n","Text from page 677:\n","Text from page 678:\n","Text from page 679:\n","Text from page 680:\n","Text from page 681:\n","Text from page 682:\n","Text from page 683:\n","Text from page 684:\n","Text from page 685:\n","Text from page 686:\n","Text from page 687:\n","Text from page 688:\n","Text from page 689:\n","Text from page 690:\n","Text from page 691:\n","Text from page 692:\n","Text from page 693:\n","Text from page 694:\n","Text from page 695:\n","Text from page 696:\n","Text from page 697:\n","Text from page 698:\n","Text from page 699:\n","Text from page 700:\n","Text from page 701:\n","Text from page 702:\n","Text from page 703:\n","Text from page 704:\n","Text from page 705:\n","Text from page 706:\n","Text from page 707:\n","Text from page 708:\n","Text from page 709:\n","Text from page 710:\n","Text from page 711:\n","Text from page 712:\n","Text from page 713:\n","Text from page 714:\n","Text from page 715:\n","Text from page 716:\n","Text from page 717:\n","Text from page 718:\n","Text from page 719:\n","Text from page 720:\n","Text from page 721:\n","Text from page 722:\n","Text from page 723:\n","Text from page 724:\n","Text from page 725:\n","Text from page 726:\n","Text from page 727:\n","Text from page 728:\n","Text from page 729:\n","Text from page 730:\n","Text from page 731:\n","Text from page 732:\n","Text from page 733:\n","Text from page 734:\n","Text from page 735:\n","Text from page 736:\n","Text from page 737:\n","Text from page 738:\n","Text from page 739:\n","Text from page 740:\n","Text from page 741:\n","Text from page 742:\n","Text from page 743:\n","Text from page 744:\n","Text from page 745:\n","Text from page 746:\n","Text from page 747:\n","Text from page 748:\n","Text from page 749:\n","Text from page 750:\n","Text from page 751:\n","Text from page 752:\n","Text from page 753:\n","Text from page 754:\n","Text from page 755:\n","Text from page 756:\n","Text from page 757:\n"]},{"output_type":"execute_result","data":{"text/plain":["'SL ARR HEAD CRACKER 170G 180125-130225 CLAIM SALE\\n$69.00\\nTHIS IS A COMPUTER GENERATED DOCUMENT.\\nNO SIGNATURE IS REQUIRED.\\n$69.00\\n$6.21\\n$75.21\\nSub Total\\nGST 9%\\nTotal\\n \\nSGD\\nDEBIT NOTE (TAX INVOICE)\\nBLK 11 UPPER BOON KENG ROAD #01-901\\n SINGAPORE 380011\\nTEL: 6747 2780  FAX: -\\nUEN: 198304925E GST NO: M2-0065333-5\\nSHENG SIONG SUPERMARKET PTE LTD\\nVendor Doc. No.\\nRef. No.\\nDoc. Date\\nPayment Term\\nCurrency\\nDepartment\\nPrepared By\\nInternal Ref.\\nSCP2502129100\\nSCP2502129100\\n27/02/25\\nCS\\nSGD\\nGROCERY\\nAPI-STD\\nYP951\\nPage\\n1\\nOutlet\\nUBK011\\nVendor No.\\nSLF\\nSING LONG FOODSTUFF TRADING CO. PTE LTD\\n12 WOODLANDS LINK\\n \\nSINGAPORE 738740\\nAmount\\nDescription\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["\n","\n","import PyPDF2\n","\n","# Replace 'CreditNoteAll_PS250300014SLF20.pdf' with the actual path to your PDF file\n","pdf_file_path = 'CreditNoteAll_PS250300014SLF20.pdf'\n","\n","with open(pdf_file_path, 'rb') as pdf_file:\n","  pdf_reader = PyPDF2.PdfReader(pdf_file)\n","  num_pages = len(pdf_reader.pages)\n","\n","  page_count = 0\n","  for page_num in range(num_pages):\n","    page = pdf_reader.pages[page_num]\n","    text = page.extract_text()\n","    if 'SCP' in text or 'SDP' in text:\n","      page_count += 1\n","\n","print(f\"Total pages containing 'SCP' or 'SDP': {page_count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKf1G1mLpH1o","executionInfo":{"status":"ok","timestamp":1742953104487,"user_tz":-480,"elapsed":7222,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"3c04acba-21d9-421f-ff8d-44f700fb6679"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Total pages containing 'SCP' or 'SDP': 579\n"]}]},{"cell_type":"code","source":["\n","import PyPDF2\n","\n","# Replace 'CreditNoteAll_PS250300014SLF20.pdf' with the actual path to your PDF file\n","pdf_file_path = 'CreditNoteAll_PS250300014SLF20.pdf'\n","\n","with open(pdf_file_path, 'rb') as pdf_file:\n","  pdf_reader = PyPDF2.PdfReader(pdf_file)\n","  num_pages = len(pdf_reader.pages)\n","\n","  page_count = 0\n","  for page_num in range(num_pages):\n","    page = pdf_reader.pages[page_num]\n","    text = page.extract_text()\n","    if 'SCP' in text:\n","      page_count += 1\n","\n","print(f\"Total pages containing 'SCP': {page_count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ap7WblD_rXXh","executionInfo":{"status":"ok","timestamp":1742956751196,"user_tz":-480,"elapsed":4141,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"65fc8c20-0130-41ea-a011-98f5687ec030"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Total pages containing 'SCP': 497\n"]}]},{"cell_type":"code","source":["\n","\n","# Replace 'CreditNoteAll_PS250300014SLF20.pdf' with the actual path to your PDF file\n","pdf_file_path = 'CreditNoteAll_PS250300014SLF20.pdf'\n","\n","with open(pdf_file_path, 'rb') as pdf_file:\n","  pdf_reader = PyPDF2.PdfReader(pdf_file)\n","  num_pages = len(pdf_reader.pages)\n","\n","  page_count = 0\n","  for page_num in range(num_pages):\n","    page = pdf_reader.pages[page_num]\n","    text = page.extract_text()\n","    if 'SDP' in text:\n","      page_count += 1\n","\n","print(f\"Total pages containing 'SDP': {page_count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gc-X8-l0roqa","executionInfo":{"status":"ok","timestamp":1742953119154,"user_tz":-480,"elapsed":4427,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"59905b9e-2bf7-424a-8403-a34397d7016c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Total pages containing 'SDP': 82\n"]}]},{"cell_type":"code","source":["import PyPDF2\n","import re\n","\n","# Replace 'CreditNoteAll_PS250300014SLF20.pdf' with the actual path to your PDF file\n","pdf_file_path = 'CreditNoteAll_PS250300014SLF20.pdf'\n","\n","total_amount = 0\n","with open(pdf_file_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    num_pages = len(pdf_reader.pages)\n","\n","    for page_num in range(num_pages):\n","        page = pdf_reader.pages[page_num]\n","        text = page.extract_text()\n","\n","        if 'SDP' in text:\n","            # Find the last dollar amount on the page\n","            matches = re.findall(r'\\$\\s*([\\d,.]+)', text)\n","            if matches:\n","                last_amount_str = matches[-1]\n","                try:\n","                    last_amount = float(last_amount_str.replace(',', ''))\n","                    total_amount += last_amount\n","                except ValueError:\n","                    print(f\"Warning: Could not convert '{last_amount_str}' to a float on page {page_num + 1}\")\n","\n","print(f\"Total sum of the last dollar amounts on pages containing 'SDP': ${total_amount}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QZyNOBHwLnZ","executionInfo":{"status":"ok","timestamp":1742953127078,"user_tz":-480,"elapsed":5172,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"e9f4b302-3132-4c78-ecd7-b9c38ee9192b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sum of the last dollar amounts on pages containing 'SDP': $19346.39999999999\n"]}]},{"cell_type":"code","source":["# prompt: sum the last $ amount which page consists 'SDP' and 'DISP'\n","\n","import PyPDF2\n","import re\n","\n","# Replace 'CreditNoteAll_PS250300014SLF20.pdf' with the actual path to your PDF file\n","pdf_file_path = 'CreditNoteAll_PS250300014SLF20.pdf'\n","\n","total_amount = 0\n","with open(pdf_file_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    num_pages = len(pdf_reader.pages)\n","\n","    for page_num in range(num_pages):\n","        page = pdf_reader.pages[page_num]\n","        text = page.extract_text()\n","\n","        if 'SDP' in text and 'DISP' in text:\n","            # Find the last dollar amount on the page\n","            matches = re.findall(r'\\$\\s*([\\d,.]+)', text)\n","            if matches:\n","                last_amount_str = matches[-1]\n","                try:\n","                    last_amount = float(last_amount_str.replace(',', ''))\n","                    total_amount += last_amount\n","                except ValueError:\n","                    print(f\"Warning: Could not convert '{last_amount_str}' to a float on page {page_num + 1}\")\n","\n","print(f\"Total sum of the last dollar amounts on pages containing 'SDP' and 'DISP': ${total_amount}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8vYJYYWxfrS","executionInfo":{"status":"ok","timestamp":1742461165266,"user_tz":-480,"elapsed":5382,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"eb24977c-2373-41d7-8b23-561d71eb7618"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sum of the last dollar amounts on pages containing 'SDP' and 'DISP': $981.0\n"]}]},{"cell_type":"code","source":["\n","\n","import PyPDF2\n","import re\n","\n","# Replace 'CreditNoteAll_PS250300014SLF20.pdf' with the actual path to your PDF file\n","pdf_file_path = 'CreditNoteAll_PS250300014SLF20.pdf'\n","\n","total_amount = 0\n","with open(pdf_file_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    num_pages = len(pdf_reader.pages)\n","\n","    for page_num in range(num_pages):\n","        page = pdf_reader.pages[page_num]\n","        text = page.extract_text()\n","\n","        if 'SCP' in text:\n","            # Find the last dollar amount on the page\n","            matches = re.findall(r'\\$\\s*([\\d,.]+)', text)\n","            if matches:\n","                last_amount_str = matches[-1]\n","                try:\n","                    last_amount = float(last_amount_str.replace(',', ''))\n","                    total_amount += last_amount\n","                except ValueError:\n","                    print(f\"Warning: Could not convert '{last_amount_str}' to a float on page {page_num + 1}\")\n","\n","print(f\"Total sum of the last dollar amounts on pages containing 'SCP': ${total_amount}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742460151357,"user_tz":-480,"elapsed":4297,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"e02bbb5f-e9e4-482a-c40c-4671fda156cc","id":"xgXHSNRxt3m8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total sum of the last dollar amounts on pages containing 'SCP': $176068.62\n"]}]},{"cell_type":"code","source":["# prompt: IMPORT THE 'CreditNoteAll_PS250300014SLF20.pdf' AND GENERATE A NEW PDF FILE WHICH THE PAGE CONTAINING OF 'SDP'\n","\n","import PyPDF2\n","\n","# Replace 'CreditNoteAll_PS250300014SLF20.pdf' with the actual path to your PDF file\n","pdf_file_path = 'CreditNoteAll_PS250300014SLF20.pdf'\n","\n","with open(pdf_file_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    num_pages = len(pdf_reader.pages)\n","\n","    pdf_writer = PyPDF2.PdfWriter()\n","\n","    for page_num in range(num_pages):\n","        page = pdf_reader.pages[page_num]\n","        text = page.extract_text()\n","\n","        if 'SDP' in text:\n","            pdf_writer.add_page(page)\n","\n","    with open('SDP_Pages.pdf', 'wb') as output_pdf:\n","        pdf_writer.write(output_pdf)\n","\n","print(\"New PDF file 'SDP_Pages.pdf' created with pages containing 'SDP'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dQz-m6gCGjX8","executionInfo":{"status":"ok","timestamp":1742953259698,"user_tz":-480,"elapsed":4144,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"e8497768-a46c-4156-b9e0-27ef28549966"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["New PDF file 'SDP_Pages.pdf' created with pages containing 'SDP'.\n"]}]},{"cell_type":"code","source":["# prompt: IMPORT THE 'CreditNoteAll_PS250300014SLF20.pdf' AND GENERATE A NEW PDF FILE WHICH THE PAGE CONTAINING OF 'SCP'\n","\n","import PyPDF2\n","\n","# Replace 'CreditNoteAll_PS250300014SLF20.pdf' with the actual path to your PDF file\n","pdf_file_path = 'CreditNoteAll_PS250300014SLF20.pdf'\n","\n","with open(pdf_file_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    num_pages = len(pdf_reader.pages)\n","\n","    pdf_writer = PyPDF2.PdfWriter()\n","\n","    for page_num in range(num_pages):\n","        page = pdf_reader.pages[page_num]\n","        text = page.extract_text()\n","\n","        if 'SCP' in text:\n","            pdf_writer.add_page(page)\n","\n","    with open('SCP_Pages.pdf', 'wb') as output_pdf:\n","        pdf_writer.write(output_pdf)\n","\n","print(\"New PDF file 'SCP_Pages.pdf' created with pages containing 'SCP'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NxeEW4SkHNUb","executionInfo":{"status":"ok","timestamp":1742953330745,"user_tz":-480,"elapsed":4356,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"28b4731f-f6fc-41dd-a34e-90c42f494a88"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["New PDF file 'SCP_Pages.pdf' created with pages containing 'SCP'.\n"]}]},{"cell_type":"code","source":["# prompt: import SS_data.csv, merge the address, address 2, address 3 columns into 1 new column and remove the address, address 2, address 3 columns in SS_data.csv and generate new csv file.\n","\n","import pandas as pd\n","\n","# Replace 'SS_data.csv' with the actual path to your CSV file\n","csv_file_path = 'SS_data.csv'\n","\n","# Read the CSV file into a pandas DataFrame\n","df = pd.read_csv(csv_file_path)\n","\n","# Merge 'address', 'address 2', and 'address 3' columns into a new column 'full_address'\n","df['full_address'] = df['Address'].fillna('') + ' ' + df['Address 2'].fillna('') + ' ' + df['Address 3'].fillna('')\n","\n","# Remove the original address columns\n","df = df.drop(['Address', 'Address 2', 'Address 3'], axis=1)\n","\n","# Save the modified DataFrame to a new CSV file\n","new_csv_file_path = 'SS_data_new.csv'\n","df.to_csv(new_csv_file_path, index=False)\n","\n","print(f\"New CSV file '{new_csv_file_path}' created with merged addresses.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OACGL9J-I2aU","executionInfo":{"status":"ok","timestamp":1742954384533,"user_tz":-480,"elapsed":32,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"89701db8-5dfe-403d-e978-8a679d8098a5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["New CSV file 'SS_data_new.csv' created with merged addresses.\n"]}]},{"cell_type":"code","source":["# prompt: import SS_data_new.csv then split the 6 digit number in the full_address column into new column named postage_code\n","\n","import pandas as pd\n","\n","# Replace 'SS_data_new.csv' with the actual path to your CSV file\n","csv_file_path = 'SS_data_new.csv'\n","\n","# Read the CSV file into a pandas DataFrame\n","df = pd.read_csv(csv_file_path)\n","\n","# Extract the 6-digit postcode from the 'full_address' column\n","df['postage_code'] = df['full_address'].str.extract(r'(\\d{6})')\n","\n","# Save the modified DataFrame to a new CSV file (optional)\n","new_csv_file_path = 'SS_data_new_with_postcode.csv'\n","df.to_csv(new_csv_file_path, index=False)\n","\n","print(f\"New CSV file '{new_csv_file_path}' created with postage_code column.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rX675jiYNITN","executionInfo":{"status":"ok","timestamp":1742954942929,"user_tz":-480,"elapsed":54,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"aae17369-ada6-4295-bb75-1cec5d42a0b1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["New CSV file 'SS_data_new_with_postcode.csv' created with postage_code column.\n"]}]},{"cell_type":"code","source":["# prompt: import SCP_Pages.pdf print the 6 digit number after SINGAPORE with the last line of '$' export the print as csv file with consists 2 columns, postcode (6 digit) and $\n","\n","import PyPDF2\n","import re\n","import pandas as pd\n","\n","# Replace 'SCP_Pages.pdf' with the actual path to your PDF file\n","pdf_file_path = 'SCP_Pages.pdf'\n","\n","data = []\n","with open(pdf_file_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    num_pages = len(pdf_reader.pages)\n","\n","    for page_num in range(num_pages):\n","        page = pdf_reader.pages[page_num]\n","        text = page.extract_text()\n","\n","        # Find the 6-digit number after \"SINGAPORE\"\n","        match = re.search(r'SINGAPORE\\s*(\\d{6})', text)\n","        if match:\n","            postcode = match.group(1)\n","\n","            # Find the last line with '$'\n","            lines = text.splitlines()\n","            for line in reversed(lines):\n","                if '$' in line:\n","                    dollar_amount = line.strip()\n","                    data.append([postcode, dollar_amount])\n","                    break\n","\n","\n","# Create a pandas DataFrame from the extracted data\n","df = pd.DataFrame(data, columns=['postcode', '$'])\n","\n","# Save the DataFrame to a CSV file\n","df.to_csv('extracted_data.csv', index=False)\n","\n","print(\"CSV file 'extracted_data.csv' created with postcode and $ columns.\")"],"metadata":{"id":"oxxmMBVfYZxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: import SS_data_new_with_postcode.csv and extracted_data.csv then create a new csv file which the file consists the postcode column (from extracted_data.csv), $ column (from extracted_data.csv), No. column (from SS_data_new_with_postcode.csv), Salesperson Code (from SS_data_new_with_postcode.csv), full_address (from SS_data_new_with_postcode.csv) when the postcode column values (extracted_data.csv) == postage_code values (SS_data_new_with_postcode.csv)\n","\n","import pandas as pd\n","\n","# Load the two CSV files into pandas DataFrames\n","df_ss_data = pd.read_csv('SS_data_new_with_postcode.csv')\n","df_extracted_data = pd.read_csv('extracted_data.csv')\n","\n","# Merge the DataFrames based on the postcode and postage_code columns\n","merged_df = pd.merge(df_extracted_data, df_ss_data, left_on='postcode', right_on='postage_code', how='inner')\n","\n","# Select only the desired columns\n","new_df = merged_df[['postcode', '$', 'No.', 'Salesperson Code', 'full_address']]\n","\n","# Save the new DataFrame to a CSV file\n","new_df.to_csv('merged_data.csv', index=False)\n","\n","print(\"New CSV file 'merged_data.csv' created.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lt_ugrkWRadN","executionInfo":{"status":"ok","timestamp":1742956638039,"user_tz":-480,"elapsed":33,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"324acea8-c55a-45e7-a5f7-00e2e534ec8e"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["New CSV file 'merged_data.csv' created.\n"]}]},{"cell_type":"code","source":["# prompt: import merged_data.csv then change the datatype for $ column which remove the $ symbol for the value\n","\n","import pandas as pd\n","\n","# Replace 'merged_data.csv' with the actual path to your CSV file\n","csv_file_path = 'merged_data.csv'\n","\n","# Read the CSV file into a pandas DataFrame\n","df = pd.read_csv(csv_file_path)\n","\n","# Remove the '$' symbol from the '$' column and convert the values to float\n","df['$'] = df['$'].str.replace('$', '', regex=False).str.replace(',', '').astype(float)\n","\n","\n","# Save the modified DataFrame to a new CSV file (optional)\n","new_csv_file_path = 'merged_data_modified.csv'\n","df.to_csv(new_csv_file_path, index=False)\n","\n","print(f\"New CSV file '{new_csv_file_path}' created with modified '$' column.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ik8RJYSUudY","executionInfo":{"status":"ok","timestamp":1742956907932,"user_tz":-480,"elapsed":17,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"72723fca-d211-4989-a1bf-e95da111c65a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["New CSV file 'merged_data_modified.csv' created with modified '$' column.\n"]}]},{"cell_type":"code","source":["# prompt: import SDP_Pages.pdf print the 6 digit number after SINGAPORE with the last line of '$' export the print as csv file with consists 2 columns, postcode (6 digit) and $ when the pages is not containing 'DISP'\n","\n","import PyPDF2\n","import re\n","import pandas as pd\n","\n","# Replace 'SDP_Pages.pdf' with the actual path to your PDF file\n","pdf_file_path = 'SDP_Pages.pdf'\n","\n","data = []\n","with open(pdf_file_path, 'rb') as pdf_file:\n","    pdf_reader = PyPDF2.PdfReader(pdf_file)\n","    num_pages = len(pdf_reader.pages)\n","\n","    for page_num in range(num_pages):\n","        page = pdf_reader.pages[page_num]\n","        text = page.extract_text()\n","\n","        if 'DISP' not in text:\n","            # Find the 6-digit number after \"SINGAPORE\"\n","            match = re.search(r'SINGAPORE\\s*(\\d{6})', text)\n","            if match:\n","                postcode = match.group(1)\n","\n","                # Find the last line with '$'\n","                lines = text.splitlines()\n","                for line in reversed(lines):\n","                    if '$' in line:\n","                        dollar_amount = line.strip()\n","                        data.append([postcode, dollar_amount])\n","                        break\n","\n","\n","# Create a pandas DataFrame from the extracted data\n","df = pd.DataFrame(data, columns=['postcode', '$'])\n","\n","# Save the DataFrame to a CSV file\n","df.to_csv('extracted_data_from_SDP.csv', index=False)\n","\n","print(\"CSV file 'extracted_data_from_SDP.csv' created with postcode and $ columns.\")"],"metadata":{"id":"4vXjx104aVz5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: import SS_data_new_with_postcode.csv and extracted_data.csv then create a new csv file which the file consists the postcode column (from extracted_data.csv), $ column (from extracted_data.csv), No. column (from SS_data_new_with_postcode.csv), Salesperson Code (from SS_data_new_with_postcode.csv), full_address (from SS_data_new_with_postcode.csv) when the postcode column values (extracted_data.csv) == postage_code values (SS_data_new_with_postcode.csv)\n","\n","import pandas as pd\n","\n","# Load the two CSV files into pandas DataFrames\n","df_ss_data = pd.read_csv('SS_data_new_with_postcode.csv')\n","df_extracted_data = pd.read_csv('extracted_data_from_SDP.csv')\n","\n","# Merge the DataFrames based on the postcode and postage_code columns\n","merged_df = pd.merge(df_extracted_data, df_ss_data, left_on='postcode', right_on='postage_code', how='inner')\n","\n","# Select only the desired columns\n","new_df = merged_df[['postcode', '$', 'No.', 'Salesperson Code', 'full_address']]\n","\n","# Save the new DataFrame to a CSV file\n","new_df.to_csv('merged_data_SDP.csv', index=False)\n","\n","print(\"New CSV file 'merged_data_SDP.csv' created.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J1YUPfudaYvh","executionInfo":{"status":"ok","timestamp":1742958404592,"user_tz":-480,"elapsed":14,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"e47062d8-eda9-4f51-e0cd-a6d466f0798c"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["New CSV file 'merged_data_SDP.csv' created.\n"]}]},{"cell_type":"code","source":["# prompt: import merged_data.csv then change the datatype for $ column which remove the $ symbol for the value\n","\n","import pandas as pd\n","\n","# Replace 'merged_data.csv' with the actual path to your CSV file\n","csv_file_path = 'merged_data_SDP.csv'\n","\n","# Read the CSV file into a pandas DataFrame\n","df = pd.read_csv(csv_file_path)\n","\n","# Remove the '$' symbol from the '$' column and convert the values to float\n","df['$'] = df['$'].str.replace('$', '', regex=False).str.replace(',', '').astype(float)\n","\n","\n","# Save the modified DataFrame to a new CSV file (optional)\n","new_csv_file_path = 'merged_data_modified_SDP.csv'\n","df.to_csv(new_csv_file_path, index=False)\n","\n","print(f\"New CSV file '{new_csv_file_path}' created with modified '$' column.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GSeT3XnSayhi","executionInfo":{"status":"ok","timestamp":1742958464102,"user_tz":-480,"elapsed":14,"user":{"displayName":"ar","userId":"10083545604967722707"}},"outputId":"170b18b7-cc15-4718-9223-06746e90657d"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["New CSV file 'merged_data_modified_SDP.csv' created with modified '$' column.\n"]}]}]}